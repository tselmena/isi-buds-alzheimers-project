---
title: "07-25"
format: html
---

```{r}
library(dplyr)
library(nlme)
library(splines)
library(emmeans)
library(ggplot2)
library(clubSandwich)
library(scales)
library(ggrepel)
```

```{r}
LEARN_raw <- 
  ADQS_raw |> 
  filter(SUBSTUDY %in% "LEARN")
```

```{r}
get_data_frames_learn <- function(data, test_code, target_weeks_w_0 , window_weeks, baseline_window_weeks) {
  test_data <- data |>
    filter(toupper(QSTESTCD) == test_code) |>
    select(BID, WEEK = ADURW, SCORE = QSSTRESN) |>
    filter(!is.na(WEEK), !is.na(SCORE))

  if (nrow(test_data) == 0) {
    # Return NULL if no data for this test code
    return(NULL)
  }

  closest <- test_data |>
    # Create a temporary list-column with the absolute difference to each target week
    mutate(
      tmp = map(WEEK, ~ abs(.x - target_weeks_w_0 )),
      # Find the target week with the minimum difference
      target_week = map_int(tmp, ~ target_weeks_w_0 [which.min(.x)])
    ) |>
    select(-tmp) |>
    # Filter to keep only visits within the specified window of the assigned target week
    filter(abs(WEEK - target_week) <= if_else(target_week <= 0, baseline_window_weeks, window_weeks)) |>
    group_by(BID, target_week) |>
    # For each subject and target week, keep the visit with the smallest time difference
    slice_min(order_by = abs(WEEK - target_week), n = 1, with_ties = FALSE) |>
    ungroup()

  baseline_scores <- closest |>
    filter(target_week <= 0) |>
    group_by(BID) |>
    # If multiple baseline candidates, take the one with the latest date
    slice_max(order_by = target_week, n = 1, with_ties = FALSE) |>
    ungroup() |>
    select(BID, baseline_score = SCORE)

  wide_followup_scores <- closest |>
    filter(target_week > 0) |>
    select(BID, target_week, SCORE) |>
    pivot_wider(
      id_cols = BID,
      names_from = target_week,
      values_from = SCORE,
      names_prefix = paste0(test_code, "_W")
    )

  wide_scores_with_deltas <- wide_followup_scores |>
    inner_join(baseline_scores, by = "BID") |>
    mutate(across(
      .cols = starts_with(paste0(test_code, "_W")),
      .fns = ~ .x - baseline_score,
      .names = "delta_{.col}"
    ))
  
  model_data <- wide_scores_with_deltas |>
    mutate(across(
      .cols = starts_with("delta_"),
      .fns = ~ as.numeric(scale(.x)),
      .names = "z_{.col}"
    ))
  
  return(model_data)
}
```

```{r}
# code in our parameters
test_codes_adqs <- c("ADLPQPT", "ADLPQSP", "ADLTOTAL", "CFIPT", "CFISP", "CFITOTAL", "DIGIT", "FCSRT96", "LMIIA", "MMSE", "PACC")
target_weeks_w_0 <- c(0, 48, 108, 168, 204, 240)
learn_list <- map(test_codes_adqs, ~get_data_frames_learn(
  data = LEARN_raw,
  test_code = .x,
  target_weeks = target_weeks_w_0,
  window_weeks = window_weeks,
  baseline_window_weeks = baseline_window_weeks
))
```

```{r}
c3_clean_learn <- C3_raw |>
  filter(SUBSTUDY %in% "LEARN") |> 
  select(BID, C3Comp, VISCODE) |>
  na.omit() |>
  distinct(BID, VISCODE, .keep_all = TRUE)

c3_wide_learn <- c3_clean_learn |> 
  pivot_wider(
    id_cols = BID,
    names_from = VISCODE,
    values_from = C3Comp,
    names_prefix = "VISCODE_"
  )

c3_learn_interp <- c3_wide_learn|>
  mutate(
    # week 0: VISCODE_003 (Baseline)
    baseline_score = VISCODE_003,
    # week 48: average of week 24 (VISCODE_012) and week 72 (VISCODE_024)
    C3_W48 = (VISCODE_012 + VISCODE_024) / 2,
    # week 108: average of week 96 (VISCODE_030) and week 120 (VISCODE_036)
    C3_W108 = (VISCODE_030 + VISCODE_036) / 2,
    # week 168: VISCODE_048
    C3_W168 = VISCODE_048,
    # week 204: average of week 192 (VISCODE_054) and week 216 (VISCODE_060)
    C3_W204 = (VISCODE_054 + VISCODE_060) / 2,
    # week 240: VISCODE_066
    C3_W240 = VISCODE_066
  ) |> 
  select(BID, baseline_score, starts_with("C3_W")) |> 
  mutate(
    delta_C3_W48 = C3_W48 - baseline_score,
    delta_C3_W108 = C3_W108 - baseline_score,
    delta_C3_W168 = C3_W168 - baseline_score,
    delta_C3_W204 = C3_W204 - baseline_score,
    delta_C3_W240 = C3_W240 - baseline_score
  )
c3_learn_interp
```

```{r}
names(learn_list) <- test_codes_adqs
learn_list$C3 <- c3_learn_interp
all_test_codes <- names(learn_list)
```

## Calculate delta, sigma-squared, and n

```{r}
delta_48 <- map2(learn_list, all_test_codes, ~ .x[[paste0("delta_", .y, "_W48")]])

delta_avg_48 <- map2_dbl(learn_list, all_test_codes,
  ~ mean(.x[[paste0("delta_", .y, "_W48")]], na.rm = TRUE)
)

learn_adjust <- as_tibble_row(delta_avg_48) |> 
  select(
    -ADLPQSP, -ADLTOTAL, -DIGIT, -FCSRT96, -PACC
  ) |> 
  mutate(
    CFIPT = -CFIPT,
    CFISP = -CFISP, 
    CFITOTAL = -CFITOTAL
  )

adj_test_list <- imap(all_tests_list, \(df, test_name) {
  if (test_name %in% names(learn_adjust)) {
    adjustment_value <- learn_adjust[[test_name]]
    target_col <- paste0("delta_", test_name, "_W48")
    new_col_name <- paste0("adj_delta_", test_name, "_W48")
    df |>
      mutate(
        "{new_col_name}" := .data[[target_col]] - adjustment_value
      )
  } else {
    df
  }
})

var_delta_df <- imap_dfr(adj_test_list, \(df, test_name) {
  adj_col <- paste0("adj_delta_", test_name, "_W48")
  if (adj_col %in% names(df)) {
    col_to_use <- adj_col
  } else {
    col_to_use <- paste0("delta_", test_name, "_W48")
  }
  tibble(
    test_name = test_name,
    var = var(df[[col_to_use]], na.rm = TRUE), 
    mean = mean(df[[col_to_use]], na.rm = TRUE)
  )
})

get_sample_size <- function(data, alpha = 0.05, beta = 0.2) {
  sigma_squared <- data$var
  delta <- data$mean
  z_alpha <- qnorm(1 - alpha / 2)
  z_beta <- qnorm(1 - beta)
  n <- ( (z_alpha + z_beta)^2 * 2 * sigma_squared ) / delta^2
  sample_size <- 
    tibble(
      test_name = data$test_name, 
      n = ceiling(n)
    ) |> 
    arrange(n)
  sample_size
}

# calculate sample size 
sample_sizes <- get_sample_size(var_delta_df, alpha = 0.05, beta = 0.2)
``` 

## AUC

```{r}
compute_auc <- function(y_true, y_score, n_cutpoints = 100) {
  if (!all(y_true %in% c(0, 1))) stop("y_true must be binary (0/1)")
  
  cutpt <- seq(max(y_score), min(y_score), length.out = n_cutpoints)
  sensitivity <- specificity <- numeric(length(cutpt))
  
  for (i in seq_along(cutpt)) {
    prog <- y_score[y_true == 1]
    noprog <- y_score[y_true == 0]
    
    sensitivity[i] <- sum(prog >= cutpt[i]) / length(prog)
    specificity[i] <- sum(noprog < cutpt[i]) / length(noprog)
  }
  
  x <- 1 - specificity
  y <- sensitivity
  auc <- sum(diff(x) * (head(y, -1) + tail(y, -1)) / 2)
  return(auc)
}

get_w48_aucs <- function(df, outcome_var = "CDPOS_W240") {
  z_delta_col <- str_subset(names(df), "^z_delta_.*_W48$")
  
  model_data_base <- df |>
    ungroup() |>
    select(all_of(outcome_var), baseline_score, all_of(z_delta_col)) |>
    na.omit()
  
  glm_base <- glm(
    paste(outcome_var, "~", z_delta_col, "+ baseline_score"),
    data = model_data_base, family = binomial()
  )
  
  y_true_base <- model_data_base[[outcome_var]]
  y_score_base <- predict(glm_base, type = "response")
  auc_with_baseline <- compute_auc(y_true_base, y_score_base)

  model_data_no_base <- df |>
    ungroup() |>
    select(all_of(outcome_var), all_of(z_delta_col)) |>
    na.omit()
  
  glm_no_base <- glm(
    paste(outcome_var, "~", z_delta_col),
    data = model_data_no_base, family = binomial()
  )
  
  y_true_no_base <- model_data_no_base[[outcome_var]]
  y_score_no_base <- predict(glm_no_base, type = "response")
  auc_without_baseline <- compute_auc(y_true_no_base, y_score_no_base)

  tibble(
    with_baseline_auc = auc_with_baseline,
    without_baseline_auc = auc_without_baseline
  )
}
```

```{r}
auc_comparison_df <- map_dfr(all_tests_list, get_w48_aucs, .id = "test_name")

auc_comparison_df <- auc_comparison_df |> 
  arrange(desc(with_baseline_auc))

auc_sample_size_df <- data.frame(
  sample_sizes, 
  auc = auc_comparison_df$with_baseline_auc
)

summary_df_48 <- 
  left_join(var_delta_df, auc_sample_size_df) |> 
  mutate(
    auc_with_baseline = auc, 
    auc_without_baseline = auc_comparison_df$without_baseline_auc
  ) |> 
  arrange(desc(n))
View(summary_df_48)
```

```{r}
plot_data <- auc_sample_size_df |>
  arrange(desc(auc)) |>
  mutate(test_name = factor(test_name, levels = test_name))

alpha <- 0.05
power <- 0.80
two_tailed <- TRUE


ggplot(plot_data, aes(x = test_name, y = n, fill = auc)) +
  geom_col(color = "black", width = 0.7) +
  
  # 3) Make n labels readable (black) and abbreviated (e.g., 3.7k)
  geom_text(
    aes(label = label_number(accuracy = 0.1, scale_cut = cut_short_scale())(n)), 
    vjust = -0.5, 
    color = "black", # Changed from "white"
    size = 4,
    fontface = "bold"
  ) +
  
  # 4) Change y-axis labels to use abbreviations (e.g., 1K, 10K)
  scale_y_log10(
    name = "Required Sample Size (n) - Log Scale",
    labels = label_number(accuracy = 1, scale_cut = cut_short_scale())
  ) +
  
  # 1) Use a new blue/teal color gradient
  scale_fill_gradient(
    name = "AUC",
    low = "#e0f7fa",  # A light cyan
    high = "#005f73", # Your requested base color
    limits = c(0.5, 0.8)
  ) +
  
  # Set plot limits and expand the top to make room for text
  coord_cartesian(clip = "off", ylim = c(min(plot_data$n), max(plot_data$n) * 1.5)) +
  
  # Labels and Title
  labs(
    title = "Test Power Analysis",
    subtitle = "Sample size needed for 80% Power (Î± = 0.05, two-tailed)",
    x = "",
    y = "Required Sample Size (n) - Log Scale"
  ) +
  
  # Theme adjustments
  theme_minimal(base_size = 14) +
  theme(
    # 2) Tilt the x-axis labels for readability
    axis.text.x = element_text(angle = 45, hjust = 1, face = "bold"),
    axis.title.y = element_text(size = 14, face = "bold"),
    plot.title = element_text(size = 20, face = "bold", hjust = 0.5),
    plot.subtitle = element_text(size = 14, hjust = 0.5),
    legend.title = element_text(size = 14, face = "bold"),
    panel.grid.major.x = element_blank(),
    panel.grid.minor.y = element_blank()
  )
```

```{r}
ggplot(auc_sample_size_df, aes(x = n, y = auc)) +
  geom_point(color = "#005f73", size = 4, alpha = 0.6) +
  geom_text_repel(
    aes(label = test_name),
    box.padding = 0.5,
    max.overlaps = Inf, # Ensure all labels are shown
    fontface = "bold",
    size = 4
  ) +
  scale_x_log10(
    name = "Required Sample Size (n) - Log Scale",
    labels = label_number(accuracy = 1, scale_cut = cut_short_scale())
  ) +

  scale_y_continuous(
    name = "Area Under Curve (AUC)",
    limits = c(0.6, 0.8), 
    breaks = seq(0.6, 0.8, by = 0.05)
  ) +
  annotate(
    "rect", xmin = 1, xmax = 500, ymin = 0.7, ymax = 0.8,
    fill = "lightgreen", alpha = 0.2
  ) +
  annotate(
    "text", x = 50, y = 0.78, label = "Ideal Tests",
    fontface = "bold.italic", color = "darkgreen", size = 5
  ) +
  labs(
    title = "Balancing Predictive Power and Study Cost",
    subtitle = "Sample size required to achieve 80% power at Î± = 0.05"
  ) +
  theme_minimal(base_size = 14) +
  theme(
    plot.title = element_text(face = "bold", hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5, color = "gray30"),
    axis.title = element_text(face = "bold")
  )
```


```{r}
baseline_sds <- map2_dbl(learn_list, all_test_codes, ~ sd(.x[["baseline_score"]], na.rm = TRUE))

avg_changes_matrix <- sapply(target_weeks, function(week) {
  map2_dbl(
    learn_list,
    all_test_codes,
    ~ mean(.x[[paste0("delta_", .y, "_W", week)]], na.rm = TRUE)
  )
})

View(as.data.frame(average_changes_matrix))

standardized_change_matrix <- sweep(average_changes_matrix, 1, baseline_sds, FUN = "/")

summary_df <- as.data.frame(standardized_change_matrix) |> 
  set_names(paste0("Standardized_Delta_W", target_weeks)) |> 
  rownames_to_column(var = "Predictor")

View(summary_df)
```

## Making Spline Plots

```{r}
A4_PLACEBO_MMSE <- ADQS_raw |> 
  filter(TX %in% "Placebo")|>
  filter(QSTESTCD == "MMSE")|>
  rename(MMSE = QSSTRESN)|>
  select(BID, ADURW, MMSE, ASEQNCS)|>
  mutate(STUDY_GROUP = "A4 Placebo") |> 
  na.omit()

LEARN_MMSE <- ADQS_raw |>
  filter(SUBSTUDY %in% "LEARN")|>
  filter(QSTESTCD == "MMSE")|>
  rename(MMSE = QSSTRESN)|>
  select(BID, ADURW, MMSE, ASEQNCS)|>
  mutate(STUDY_GROUP = "LEARN") |> 
  na.omit()

COMBINED_MMSE <- bind_rows(A4_PLACEBO_MMSE, LEARN_MMSE) |> 
  mutate(STUDY_GROUP = factor(STUDY_GROUP))
```

```{r}
# Define Spline Functions adapted from Intro-to-A4-data.pdf
ns21_comb <- function(t){
  as.numeric(predict(splines::ns(COMBINED_MMSE$ADURW, df = 2,
                                 Boundary.knots = c(0, max(COMBINED_MMSE$ADURW))), t)[,1])
}
ns22_comb <- function(t){
  as.numeric(predict(splines::ns(COMBINED_MMSE$ADURW, df = 2,
                                 Boundary.knots = c(0, max(COMBINED_MMSE$ADURW))), t)[,2])
}

combined_fit <- gls(
  MMSE ~ (I(ns21_comb(ADURW)) + I(ns22_comb(ADURW))) * STUDY_GROUP,
  data = COMBINED_MMSE,
  weights = varIdent(form = ~ 1 | ASEQNCS),
  correlation = corAR1(form = ~ ASEQNCS | BID),
  na.action = na.omit
)
```

```{r}
# specify both study groups and set the time sequence to end at 240 weeks.
combined_emmeans <- ref_grid(
  combined_fit,
  at = list(ADURW = seq(0, 240, by = 12),
            STUDY_GROUP = c("A4 Placebo", "LEARN")),
  vcov. = clubSandwich::vcovCR(combined_fit, type = "CR2")
)|>
  emmeans(specs = "STUDY_GROUP", by = "ADURW")|>
  as_tibble()

ggplot(combined_emmeans, aes(x = ADURW, y = emmean, color = STUDY_GROUP, fill = STUDY_GROUP)) +
  geom_line(linewidth = 1) +
  geom_ribbon(aes(ymin = lower.CL, ymax = upper.CL), alpha = 0.2, linetype = 0) +
  scale_x_continuous(breaks = c(0, 48, 108, 168, 204, 240)) +
  scale_color_manual(values = c("A4 Placebo" = "#D81B60", "LEARN" = "#1E88E5")) +
  scale_fill_manual(values = c("A4 Placebo" = "#D81B60", "LEARN" = "#1E88E5")) +
  labs(
    title = "Comparison of Mean MMSE between A4 (Placebo) and LEARN Study Participants",
    subtitle = "Estimated from a GLS spline model",
    y = "Mean MMSE with 95% confidence intervals",
    x = "Weeks since Randomization",
    color = "Cohort", 
    fill = "Cohort"
  ) +
  coord_cartesian(xlim = c(0, 240)) +
  theme_light() +
  theme(legend.position = "bottom")
```

```{r}
A4_PLACEBO_LMIIa <- ADQS_raw |> 
  filter(TX %in% "Placebo")|>
  filter(QSTESTCD == "LMIIa")|>
  rename(LMIIa = QSSTRESN)|>
  select(BID, ADURW, LMIIa, ASEQNCS)|>
  mutate(STUDY_GROUP = "A4 Placebo") |> 
  na.omit()

LEARN_LMIIa <- ADQS_raw |>
  filter(SUBSTUDY %in% "LEARN")|>
  filter(QSTESTCD == "LMIIa")|>
  rename(LMIIa = QSSTRESN)|>
  select(BID, ADURW, LMIIa, ASEQNCS)|>
  mutate(STUDY_GROUP = "LEARN") |> 
  na.omit()

COMBINED_LMIIa <- bind_rows(A4_PLACEBO_LMIIa, LEARN_LMIIa) |> 
  mutate(STUDY_GROUP = factor(STUDY_GROUP))
```

```{r}
# define Spline Functions adapted from Intro-to-A4-data.pdf
ns21_comb <- function(t){
  as.numeric(predict(splines::ns(COMBINED_LMIIa$ADURW, df = 2,
                                 Boundary.knots = c(0, max(COMBINED_LMIIa$ADURW))), t)[,1])
}
ns22_comb <- function(t){
  as.numeric(predict(splines::ns(COMBINED_LMIIa$ADURW, df = 2,
                                 Boundary.knots = c(0, max(COMBINED_LMIIa$ADURW))), t)[,2])
}

combined_fit <- gls(
  LMIIa ~ (I(ns21_comb(ADURW)) + I(ns22_comb(ADURW))) * STUDY_GROUP,
  data = COMBINED_LMIIa,
  weights = varIdent(form = ~ 1 | ASEQNCS),
  correlation = corAR1(form = ~ ASEQNCS | BID),
  na.action = na.omit
)
```

```{r}
combined_emmeans <- ref_grid(
  combined_fit,
  at = list(ADURW = seq(0, 240, by = 12),
            STUDY_GROUP = c("A4 Placebo", "LEARN")),
  vcov. = clubSandwich::vcovCR(combined_fit, type = "CR2")
)|>
  emmeans(specs = "STUDY_GROUP", by = "ADURW")|>
  as_tibble()

ggplot(combined_emmeans, aes(x = ADURW, y = emmean, color = STUDY_GROUP, fill = STUDY_GROUP)) +
  geom_line(linewidth = 1) +
  geom_ribbon(aes(ymin = lower.CL, ymax = upper.CL), alpha = 0.2, linetype = 0) +
  scale_x_continuous(breaks = c(0, 48, 108, 168, 204, 240)) +
  scale_color_manual(values = c("A4 Placebo" = "#D81B60", "LEARN" = "#1E88E5")) +
  scale_fill_manual(values = c("A4 Placebo" = "#D81B60", "LEARN" = "#1E88E5")) +
  labs(
    title = "Comparison of Mean LMIIa between A4 (Placebo) and LEARN Study Participants",
    subtitle = "Estimated from a GLS spline model",
    y = "Mean LMIIa with 95% confidence intervals",
    x = "Weeks since Randomization",
    color = "Cohort", 
    fill = "Cohort"
  ) +
  coord_cartesian(xlim = c(0, 240)) +
  theme_light() +
  theme(legend.position = "bottom")
```



















